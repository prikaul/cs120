\documentclass[11pt]{article}
\usepackage{classTools}

\begin{document}

\psHeader{1}{Wed 2023-09-20 (11:59pm)}

Please review the Syllabus for information on the collaboration policy, grading scale, revisions, and late days.


\begin{enumerate}
    \item (Asymptotic Notation) 
    \begin{enumerate}
    \item (practice using asymptotic notation)
        Fill in the table below with ``T'' (for True) or ``F'' (for False) to indicate the relationship between $f$ and $g$. For example, if $f$ is $O(g)$, the first cell of the row should be ``T.'' \\
        \begin{table}[h!]
        \centering
        \bgroup
        \def\arraystretch{1.3}
        \begin{tabular}{||c | c || c | c | c | c | c ||}
         \hline
         $f$ & $g$ & $O$ & $o$ & $\Omega$ & $\omega$ & $\Theta$ \\
         \hline\hline
         $e^{n^2}$ & $e^{2n^2}$ & & & & & \\ \hline
         $n^3$ & $n^{3/n}$ & & & & & \\ \hline
         $n^{2+(-1)^n}$ & $\binom{n}{2}$ & & & & & \\ \hline
         $(\log {n})^{120}\sqrt{n}$ & $n$ & & & & & \\ \hline
         $\log(e^{n^2})$ & $\log(e^{2n^2})$ & & & & & \\ \hline
        \end{tabular}
        \egroup
        \end{table}
        Recall that, throughout CS120, all logarithms are base 2 unless otherwise specified. 
        
    \item  (rigorously reasoning about asymptotic notation)  
    For each of the following claims, either justify why the statement holds (for all $f$, $g$) or provide a counterexample. In all cases, take the domain of the functions $f$ and $g$ to be the natural numbers (rather than the positive reals), and assume $f(n), g(n)\geq 1$ for all $n$.
    \begin{itemize}
        \item For all positive integers $a$ and $b$, if $f(n) = \Theta(a^n)$ and $g(n) = \Theta(n^b)$, then $f(g(n)) = \Theta(a^{(n^b)})$.
        \item For all positive integers $a$ and $b$, if $f(n) = \Theta(a^n)$ and $g(n) = \Theta(n^b)$, then $g(f(n)) = \Theta((a^n)^b)$.
    \end{itemize}
  
    \end{enumerate}
    
    \newpage
    
    \item (Understanding computational problems and mathematical notation)\\\\
    Recall the definition of a {\em computational problem} from Lecture Notes 1.  

 
    Consider the following computational problem $\Pi=(\Inputs,\Outputs,f)$ and algorithm $\BC$ to solve it, where
    \begin{itemize}                                
    \item $\Inputs = \N\times\N\times \N$ 
    \item $\Outputs = \{(c_0,c_1,\ldots,c_{k-1}) : k,c_0,\ldots,c_{k-1}\in \N\}$
    \item $f(n,b,k) = \{ (c_0,c_1,\ldots,c_{k-1}) : n=c_0+c_1b+c_2b^2+\cdots+c_{k-1}b^{k-1}, \forall i\ 0\leq c_i< b\}.$ 
    \end{itemize}

    
\begin{algorithm}[H]
    \BC{$n,b,k$}\\
    {
    \lIf{$b<2$}{\Return{$\bot$}}
    \ForEach{$i=0,\ldots,k-1$}{
    $c_i = n \bmod b$\;
    $n = (n-c_i)/b$\;
    }
    \lIf{$n==0$}{\Return{$(c_0,c_1,\ldots,c_{k-1})$}}
    \lElse{\Return{$\bot$}}}
\end{algorithm}


\begin{enumerate}
\item If the input is $(n,b,k) = (11,10,4)$, what does the algorithm BC return? (Note that the output is not (1,1).) Is BC's output a valid solution for $\Pi$ with input $(11,10,4)$?
\item Describe the computational problem $\Pi$ in words.  (You may find it useful to try some more examples with $b=10$.) 
\item Is there any $x\in \Inputs$ for which $f(x)=\emptyset$? If so, give an example; if not, explain why.
\item For each possible input $x\in \Inputs$, what is $|f(x)|$? ($|A|$ is the size of a set $A$.) Justify your answer(s) in one or two sentences.
\item Let $\Pi'=(\Inputs,\Outputs,f')$ be the problem with the same $\Inputs$ and $\Outputs$ as $\Pi$, but $f'(n,b,k) = f(n,b,k) \cup \{(0,1, \ldots,k-1)\}$. Does every algorithm $A$ that solves $\Pi$ also solve $\Pi'$? (Hint: any differences between inputs that were relevant in the previous subproblem are worth considering here.) Justify your answer with a proof or a counterexample.


\end{enumerate}
\newpage

\item (Radix Sort) In the Sender--Receiver Exercise associated with lecture 3, you studied the sorting algorithm {\em Counting Sort}, generalized to arrays of key--value pairs, and proved that it has running time $O(n+U)$ when the keys are drawn from a universe of size $U$. In this problem you'll study {\em Radix Sort}, which improves the dependence on the universe size $U$ from linear to logarithmic.  Specifically, Radix Sort can achieve runtime $O(n+n(\log U)/(\log n))$, so it achieves runtime $O(n)$ whenever $U = n^{O(1)}$.  
Radix Sort is constructed by using Counting Sort as a subroutine several times, but on a smaller universe size $b$.
Crucially, Radix Sort uses the fact that Counting Sort can be implemented in a way that is {\em stable} in the sense that it preserves the order in the input array when the same key appears multiple times.  Here is pseudocode for Radix Sort, using the algorithm $BC$ above as a subroutine:

\begin{algorithm}[H]
\RadixSort{$U,b,A$}\\
\Input{A universe size $U\in \N$, a base $b\in \N$ with $b\geq 2$, and an array $A=((K_0,V_0),\ldots,(K_{n-1},V_{n-1}))$, where each $K_i\in [U]$}
\Output{A valid sorting of $A$}
$k=\lceil (\log U)/(\log b)\rceil$\;
\ForEach{$i=0,\ldots,n-1$}{
    $V_i' = \BC(K_i,b,k)$}
\ForEach{$j=0,\ldots,k-1$}{
    \ForEach{$i=0,\ldots,n-1$}{
    $K'_i = V'_i[j]$
    }
    $((K_0',(V_0,V'_0)),\ldots,(K_{n-1}',(V_{n-1},V'_{n-1}))) = \CountingSort(b,((K'_0,(V_0,V_0')),\ldots,(K'_{n-1},(V_{n-1},V'_{n-1})))$\;
}
\ForEach{$i=0,\ldots,n-1$}{
    $K_i = V'_i[0]+V'_i[1]\cdot b + V'_i[2]\cdot b^2+\cdots+V'_i[k-1]\cdot b^{k-1}$}
\Return{$((K_0,V_0),\ldots,(K_{n-1},V_{n-1}))$}
\caption{Radix Sort}
\end{algorithm}

(You can also read a description of Radix Sort in CLRS Section 8.3 for the case of sorting arrays of keys (without attached items) when $U$ and $b$ are powers of 2, albeit using different notation than us.)

        \begin{enumerate}
        
            \item (proving correctness of algorithms) Prove the correctness of \RadixSort\ (i.e. that it correctly solves the Sorting problem). 
            
            Hint: You will need to use the stability of \CountingSort in your argument. Note that if in the 8th line of \RadixSort algorithm, you replaced \CountingSort with ExhaustiveSearchSort (or any other sort which isn't stable), the resulting algorithm would not correctly solve sorting. 

            Here is an example (using ExhaustiveSearchSort instead of stable sort in line 8). Suppose $n=3, b=2, U=4$, $K_0=1, K_1=3, K_2=2$ and $V_0, V_1, V_2$ are ``a'', ``b'', and ``c''. Then $V'_0=(1,0), V'_1=(1,1), V'_2=(0,1)$. Suppose ExhaustiveSearchSort is such that the permutation $\pi(2)=0, \pi(1)=1, \pi(0)=2$ 
 is tried first. Sorting based on the first bit will lead to the array $(K_2=2, (c,(0,1))), (K_1=3, (b,(1,1))), (K_0=1,(a,(1,0)))$. Next, sorting the second bit using the same ExhaustiveSearchSort will give the array $(K_0=1,(a,(1,0))), (K_1=3, (b,(1,1))), (K_2=2, (c,(0,1)))$. Thus we return the same input array $((1,a),(3,c),(2,b))$!
            
            \item (analyzing runtime) Show that \RadixSort\ has runtime $O((n+b)\cdot \lceil \log_b U\rceil)$.  Set $b=\min\{n,U\}$ to obtain our desired runtime of $O(n+n(\log U)/(\log n))$.  (This runtime analysis is outlined in CLRS, but you'd need to adapt it to our notation and slightly more general setting.) 
            
            \item (implementing algorithms)
            Implement \RadixSort\ using the implementations of \CountingSort\ and \BC\ that we provide you in the GitHub repository. 
  
            \item (experimentally evaluating algorithms) Run experiments to compare the expected runtime of \CountingSort, \RadixSort (with base $b=n$), and \MergeSort\ as $n$ and $U$ vary among powers of 2 with $1\leq n\leq 2^{16}$ and $1\leq U\leq 2^{20}$.  For each pair of $(n,U)$ values you consider, run multiple trials to estimate the expected runtime over random arrays where the keys are chosen uniformly and independently from $[U]$.  For each sufficiently large value of $n$, the asymptotic (albeit worst-case) runtime analyses suggest that \CountingSort\ should be the most efficient algorithm for small values of $U$, \MergeSort\ should be the most efficient algorithm for large values of $U$, and \RadixSort\ should be the most efficient somewhere in between.  Plot the transition points from \CountingSort to \RadixSort, and \RadixSort to \MergeSort\ on a $\log n$ vs. $\log U$ scale (as usual our logarithms are base 2).  Do the shapes of the resulting transition curves fit what you'd expect from the asymptotic theory?  Explain.
            
            \textit{Note: We are expecting to see one (or more, if necessary) graphs that demonstrate, for every value of $n$, for which value of $U$ \RadixSort first outperforms \CountingSort and \MergeSort first outperforms \RadixSort. You should label the graphs appropriately (title, axis labels, etc.) and provide a caption, as well as an answer and explanation to the above question. Please look at the provided starter code for more information on generating random arrays, timing experiments, and graphing. Your implementation of RadixSort, as well as any code you write for experimentation and graphing need not be submitted.  Depending on your implementation, running the experiments could take anywhere from 15 minutes to a couple of hours, so don't leave them to the last minute!}   

          
        \end{enumerate}

\end{enumerate}
\end{document}
